\apendice{Technical Programming Documentation}

\section{Introduction}
This section contains the technical programming documentation, describing the directory structure of the presented CD, an installation guide as well as a documentation of the realized tests to measure the code quality.

\section{Directory Structure}
This report is handed in along with a DVD containing the essential data to examine the application in more depth. The presented DVD contains the following directories:
\begin{description}
\item[Application] 
Contains all essential data of the developed software, subdivided in the following directories:
\begin{description}
\item[Agent] Contains the exported API.AI agent as .zip
\item[Code] The source code of the application
\item[Javadoc] The source code’s documentation in Javadoc 
\item[VM] The virtual machine image containing the infrastructure that was set up in the course of the project
\end{description}
\item[Data] Contains the raw geographic data and dumps of the initial database 
\begin{description}
\item[Analysis] Contains the Python script to generate user ratings from a TripAdvisor data set 
\end{description}
\item[Documentation] 
Contains the project’s documentation, saved as .pdf format.
\item[Tools] 
Contains the executables of the tools needed to run the virtual machine. 
format.
\end{description}

\subsection{Source Folder Overview} \label{ss:sfo}
The application’s source folder follows the traditional maven project structure.
\begin{description}
\item[src] Source Code
\begin{description}
\item[java] Main Contains the application’s Java code, subdivided in the six packages chatbot, dataAccess, domain, messenger, recommender and service
\item[resources]  Contains user ratings that were generated previously, saved as .csv files. They are not required for the application's correct execution.
\begin{description}
\item[util] Contains the RatingsGenerator. Not included in the main folder as it is a separate program that is not required for the main application’s execution
\end{description}
\end{description}
\item[test] Test Code Contains the project's unit tests
\begin{description}
\item[chatbot] Contains, amongst others, the class TouristChatbotTest.java which contains JUnit tests that cover a system-wide functionality
\item[service] Tests covering the service package classes
\item[recommender] Contains unit tests concerning the recommender as well as the recommender evaluation seen in \ref{ss:re}
\item[dataAccess] Tests the database access, using the local PostgreSQL database in the VM
\end{description}
\end{description}

\section{APIs, Libraries and Frameworks}
Some of the used tools and technologies were already examined in the previous documentation due to their importance to the project. For the sake of completeness and overview, the following listing shows all APIs, libraries and frameworks used in the project. 
\begin{description}
\item[\hyperlink{http://mahout.apache.org/}{Apache Mahout}] The Java library is used to compute recommendations
\item[\hyperlink{https://api.ai/docs/reference/agent/query}{API.AI API}] The modeled NLU agent is accessed using the REST-like HTTP API
\item[\hyperlink{https://developer.foursquare.com/}{Foursquare API}]The API is used to access the Foursquare database and, more precisely, to retrieve image URLs
\item[\hyperlink{https://spark.apache.org/}{Spark}] A micro framework to create Java web applications 
\item[\hyperlink{https://github.com/pengrad/java-telegram-bot-api}{Telegram Bot API}] Provides a Java API for Telegram Bot API 3.0 methods
\item[\hyperlink{http://square.github.io/okhttp/}{Okhttp}] Provides an HTTP client for Java applications; used to access the Foursquare and API.AI HTTP API 
\item[\hyperlink{https://github.com/google/gson}{Google GSON}] A Java library used to deserialize JSON into Java Objects
\item[\hyperlink{https://github.com/jhalterman/expiringmap}{ExpiringMap}] Provides a Java map that expires entries; used to realize a lightweight cache that remembers currently active users that interact with the chatbot
\end{description}

\section{Developer Manual}
This section serves as an installation guide describing which steps to take to set up the application. 

\subsection{Database Setup} \label{ssec:dbs}
The geographic database is used by the chatbot application to retrieve and manage geographical data. In this project, the database runs on a Virtual Machine using Ubuntu 16.0.4 LTS. Therefore, Ubuntu’s terminal is used to install most of the software. In order to make sure Ubuntu has access to the current package index, it is advised to execute an update command before installing the software:
\begin{lstlisting}
sudo apt-get update
\end{lstlisting}
\subsubsection{Set up PostgreSQL and PostGis}
The first step is to install the data management system, PostgreSQL. To install the version used in this project, the following command is used:
\begin{lstlisting}
sudo apt-get install -y postgresql=9.5+173 postgresql-contrib=9.5+173 
\end{lstlisting}
Then, the database “touristdb” is created as well as the managing user, which is called “touristuser”. The createuser command will prompt for a password which can be chosen by the developers.
\begin{lstlisting}
sudo -u postgres createuser -P touristuser
sudo -u postgres createdb -owner touristuser touristDB
\end{lstlisting}
Now we have set up the database, the PostGIS extension is installed and added to prepare the database for geospatial data.
\begin{lstlisting}
sudo apt-get install -y postgis postgresql-9.5-postgis-2.2
sudo -u postgres psql -c "CREATE EXTENSION postgis; CREATE EXTENSION postgis_topology;" touristDB
\end{lstlisting}
The next step is optional, but seems convenient if the developers want to manage their database with the help of a user interface. The managing tool pgadmin facilates running and editing SQL queries and viewing the stored data.
\begin{lstlisting}
sudo apt-get install pgadmin3
\end{lstlisting}
To access the database in pgadmin3, a connection to the server must be added, which can be realized by clicking the plug button in the upper toolbar and then entering the following values.
\imagen{pgadmin}{pgadmin3: A server connection is added}
In the object browser, the database schemas can be viewed accessing TouristChatbot -> databases -> touristDB.

\subsubsection{Import Data into Database}
Now that we have set up the database, it needs to be filled with geospatial data. In this project, the recommendations are based on test data of Barcelona. The required data is downloaded as a .pbf file from the website https://download.bbbike.org/osm/bbbike/Barcelona/. After that, the tool Osmosis is used to import the OSM data which can be installed using the following command:
\begin{lstlisting}
sudo apt-get install osmosis
\end{lstlisting}
The next commands prepares the database for the osmosis import. It sets the hstore extension and the pgsnapshot database schema which causes that all relevant tag data are stored in a hstore column.
\begin{lstlisting}
sudo -u postgres psql -c "CREATE EXTENSION hstore;" touristDB
psql -U touristuser -d touristDB -f /usr/share/doc/osmosis/examples/pgsnapshot_schema_0.6.sql
\end{lstlisting}
After that, the import itself is realized. Remember to execute this command in the folder where the downloaded .pbf file is situated and to add the corresponding password (which is set by the developer in the previous step of this manual).
\begin{lstlisting}
osmosis --read-pbf file="Barcelona.osm.pbf" --write-pgsql host="localhost" database="touristDB" user="touristuser" password=password
\end{lstlisting}
In order to see if the import was successful, pgadmin3 can be used to take a look at the now imported data. Again, this step is optional. In the object browser on the left hand side of the user interface, the database tables can be viewed accessing \textit{TouristChatbot $\,\to\,$ databases $\,\to\,$ touristDB $\,\to\,$ Schemas $\,\to\,$ public $\,\to\,$ Tables}.

\subsubsection{Store User Information}
In order to store information of the users or of the ratings they made, the existing users table must be modified. To do so, the following POSTGRESQL queries are executed so that new columns are added to our table. These modifcations can either be made using the psql command via bash or pgamin3's query tool.
\begin{lstlisting}[language=sql]
alter table users add column recommendations bigint[];
alter table users add column unrated bigint[];
alter table users add column radius integer;
alter table users add column name ;
\end{lstlisting}

The ratings are stored in a newly created table:
\begin{lstlisting}[language=sql]
create table ratings(
userId bigint,
pointId bigint,
ratings integer,
PRIMARY KEY(userId, pointId))
\end{lstlisting}

\subsection{Access to External Services}
In the following, it is described how to set up and access the external services used in this project:
To access the conversational interface, the messenger Telegram as well as our natural language parsing platform api.ai are used. Additionally, the FourSquare API is used to retrieve images for recommended Points of Interests.

\subsubsection{Telegram Bot}
 The messenger Telegram is used to provide an interface to our tourist bot. After installing Telegram on a mobile device and setting up an account, the bot can be created using Telegram’s \textit{BotFather}. The Botfather can be accessed using the messenger’s search function. After that, the creation of the bot is triggered by entering /newbot in the input field.
\imagen{Botfather}{ The \textit{Botfather} is used to create the Telegram Bot}
After choosing a name and a username, the BotFather provides you with the authorization token for your bot. This authorization token is needed to access the Telegram bot from our web service. To do so, the token is saved as an environment variable (see \nameref{sssec:sev}). After saving the token, the web service is able to receive updates from and send messages to the Telegram bot via a webhook. 

\subsubsection{API.AI agent}
In order to use the NLU platform API.AI, we need to set up an \hyperlink{https://console.api.ai/api-client/}{account}. After doing so, the API.AI agent modeling interface can be accessed. First, we need to create a new agent and enter an agent name. 
\imagen{apiai}{API.AI agent creation interface}
If the creation is successful, the entered agent name will appear on the left sidebar. In order to access the agent from our web service, API.AI’s HTTP API is used. Therefore, the agent’s API key is needed which can be accessed by clicking on the gear icon right to the agent name. The client access token is be copied and, again, introduced into the system environment variables (see \nameref{sssec:sev}).

To restore the agent created in the course of this project, a.zip file containing the modeled agent can be found in the project’s documents (Application/Agent). By clicking on the already mentioned gear icon right to the agent name, a subtab called “Export and Import” provides the possibility to import the agent from zip.

\subsubsection{Foursquare}
To retrieve images using the Foursquare API, a Foursquare account has to be created first. After that, the application has to be \hyperlink{https://de.foursquare.com/developers/register}{registered}. If the registration is successful, the API access token can be found in the application overview. Again, these tokens are saved in the \nameref{sssec:sev}).

\subsection{Web Service Setup} 
\subsubsection{Prerequisites}
This project runs on Java 8 which is a requirement for the web service framework Java Spark as well as the used cloud service Heroku. The JDK can be downloaded from  \hyperlink{http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html}{Oracle}. 
Git is used to manage the project’s source code as well as to deploy the code to the Platform as a Service application. In order to manage a Git repository, you have to sign up on \hyperlink{https://github.com}{GitHub} and install Git using the following command.
\begin{lstlisting}
sudo apt-get install git-all  
\end{lstlisting}
On top of it, all libraries used during this project are included using the build-management tool Apache Maven. This program is installed by executing the following command in Ubuntu’s command line. Maven is also needed for deploying a Java application to Heroku.
\begin{lstlisting}
sudo apt-get install maven 
\end{lstlisting}


\subsubsection{Deployment to Heroku}
The web service is deployed using Heroku, a cloud Platform as a Service (PaaS). In order to use the application, an account has to be created previously, following https://signup.heroku.com. 
At first, the Heroku command line interface has to be installed. Using Ubuntu, this is achieved executing the following commands:
\begin{lstlisting}
sudo add-apt-repository "deb https://cli-assets.heroku.com/branches/stable/apt ./"
curl -L https://cli-assets.heroku.com/apt/release.key | sudo apt-key add - 
sudo apt-get update
sudo apt-get install heroku
\end{lstlisting}
After installing the command line, execute the following command and enter the Heroku credentials when asked.
\begin{lstlisting}
heroku login  
\end{lstlisting}
In Ubuntu's file system, change into the project directory touristbot and execute the following command in order to create a Heroku app.
\begin{lstlisting}
heroku create
\end{lstlisting}
As we can see, a random application name is assigned, in this case e.g. \href{https://arcane-fjord-43759.herokuapp.com/}{https://arcane-fjord-43759.herokuapp.com/}. This name has to be copied and inserted as our HEROKU \_ URL to the \nameref{sssec:sev}). In doing so, the Telegram bot is hooked to the Heroku app later.
\imagen{heroku_create}{Heroku Web App Creation}
The source code can be now deployed using the command:
\begin{lstlisting}
git push heroku master
\end{lstlisting}

\subsubsection{Heroku Postgres Setup}
In order to access our geospatial database online, Heroku Postgres is used to set up a productive PostgreSQL database. The following commands are executed from the Heroku repository:
\begin{lstlisting}
heroku addons:create heroku-postgresql:hobby-dev
\end{lstlisting}
After that, the psql command is used in Heroku to enable sending POSTGRESQL queries:
\begin{lstlisting}
PGUSER=postgres heroku pg:psql
\end{lstlisting}

The following queries are executed to enable the PostGis support in the PostgreSQL database.
\begin{lstlisting}[language=SQL]
CREATE EXTENSION postgis;
CREATE EXTENSION hstore;
CREATE EXTENSION postgis_topology;
\end{lstlisting}

The touristdb that was set up in the \nameref{ssec:dbs} is then pushed to the just created database:

\begin{lstlisting}
PGUSER=postgres heroku pg:push touristdb DATABASE_URL
\end{lstlisting}


\subsection{System Environment Variables}\label{sssec:sev}
In order to manage the access tokens of our external components and not push them publically into a repository, system environment variables are used. These are set differently according to whether tests are run locally on the virtual machine or the application runs productively in Heroku. To set the system environment variables locally, the file /etc/environment is modified to contain the following variables:

\begin{lstlisting}
HEROKU_URL=(*@\textit{INSERT HEROKU URL}@*)
DATABASE_URL="postgres://touristuser:(*@\textit{password}@*)@localhost:5432/touristdb"
TELEGRAM_TOKEN=(*@\textit{INSERT TELEGRAM TOKEN}@*)
API_AI_ACCESS_TOKEN=(*@\textit{INSERT API.AI CLIENT ACCESS TOKEN}@*)
F_CLIENT_ID=(*@\textit{INSERT FOURSQUARE CLIENT ID}@*)
F_CLIENT_SECRET=(*@\textit{INSERT FOURSQUARE CLIENT SECRET}@*)
\end{lstlisting}

The placeholders are replaced with the respective tokens from the external services. Concerning the variable DATABASE \_ URL, the POSTGRESQL password has to be entered that was set in the \nameref{ssec:dbs}). For the productive runtime environment, the bash is used to set the environment variables on Heroku, entering the following command:

\begin{lstlisting}
heroku config:set TOKEN_PARAMETER=VALUE
\end{lstlisting}

This command’s execution is repeated for all of the above mentioned tokens with the exception of the variable DATABASE \_ URL as it is an already predefined environment variable.

\subsection{Integrated Development Environment}
The project is developed using Eclipse Neon as an IDE. The 64-bit installer can be downloaded from \hyperlink{https://www.eclipse.org/downloads/}{Eclipse}'s website. 
After installing Maven and Eclipse, start Eclipse in order to import the source code. This can be easily done by importing a Maven project, executing \textit{File $\,\to\,$ Import $\,\to\,$ Existing Maven Projects} and then choosing the project's source folder.
 
The project’s source code can now be accessed and modified using Eclipse. Additionally, Eclipse can be used to run the Junit tests for this project.

\section{Program Compilation, Installation and Execution}
The presented application was designed for online usage and is deployed to the Platform as a Service Heroku.  Therefore, no further compilation, installation or execution steps are needed as this is managed by the PaaS. Changes to the previous source code are published by using the Git workflow, meaning to commit the changes and then push them to Heroku using the command:
\begin{lstlisting}
git push heroku master
\end{lstlisting}

In order to use Heroku, it is required to use Apache Maven as a build-management tool \cite{maven}. Maven is used to declare dependencies, compile the source code and run tests. The configuration file pom.xml (Project Object Model) defines the Maven project and is also included in the project’s documentation. To run all of the project’s tests, the following command can be used from the Maven project folder:

\begin{lstlisting}
mvn test
\end{lstlisting}

However, it must be noted that the tests also cover the database connection and access of external APIs, so the tests need to be executed in a development environment (e.g. by using the provided virtual machine) with a current Internet connection.

\section{Tests}
Tests were made during this project to ensure the project’s quality. For this reason, several measures were taken to concentrate on different aspects of quality assurance. 

\subsection{Automation Testing using JUnit}
Java’s unit testing framework JUnit is used to design automated tests. Using JUnit, the test-driven development paradigm was applied in this project to ensure the code’s correctness constantly during development. The tests can be found in the source folder tree navigating to \textit{src/test/java}. The folder is subdivided into the packages seen in \ref{ss:sfo} which match the main components in the project. In this project, two different kinds of tests were designed:
\begin{description}
\item[Unit tests] that concentrate on ensuring the proper functioning of the code on a class level. 
\item[Integration tests] covering the essential use cases of the chatbot (including all of the system’s relevant components and therefore demonstrating the proper interaction of the components). These integration tests can be found in the JUnit test class \textit{src/test/java/chatbot/TouristChatbotTest}
\end{description}

The code coverage tool \href{http://www.eclemma.org/}{EclEmma} is used to show how much of the source code is actually tested by the JUnit tests. EclEmma is integrated into the IDE Eclipse. The following results are obtained by executing all of the project’s JUnit tests:

\imagen{eclemma}{Code Coverage Analysis Result}

As we can see, 80 \% of the productive source code is tested. The test coverage mainly centers on the proper functioning of the service classes of the chatbot, meaning the classes that provide important functionalities and are error prune due to their complex structure. On the other hand, model classes are not as extensively tested as most of them follow a simple design, providing only getter, setter and field-based equals implementations. Furthermore, classes using code from external libraries are not a focus of the tests as it is assumed that their proper functioning is ensured by the developers of the respective libraries (e.g. hooking the application to the Telegram bot by using a third party Telegram library). 

\subsection{Recommender Evaluation} \label{ss:re}
To ensure that the computed recommendations of the chatbot are actually adjusted the users’ preferences, an evaluation is made using the \textit{Mahout} framework. The evaluation is limited to the user-based part of the recommender which is based on user ratings. The reason for this is that the content-based mechanism is used as a fallback that provides recommendation when the user data is too sparse for the user-based recommender to perform properly. In fact, the content-based inspired approach used in this project is not a recommender in the traditional sense but rather a similarity measure. The proper functioning of this mechanism is tested using unit tests (see the JUnit test class \textit{RecommenderTest}).

The actual recommender evaluation can be found in \textit{src/test/java/poiRecommendation/RecommenderEvaluation}. Mahout’s \textit{RecommenderIRStatsEvaluator} is used which splits the available user data automatically into training and test sets. To evaluate the recommender performance, information retrieval metrics are computed. More precisely, the metrics precision and recall are examined as well as the F-Measure which is a harmonic mean of the other mentioned metrics \cite{rijsbergen79}.

\begin{equation}
Precision=\frac{|\text{relevant items retrieved}|}{|\text{items retrieved}|}
\end{equation}
\begin{equation}
Recall=\frac{|\text{relevant items retrieved}|}{|\text{relevant items in collection}|}
\end{equation}
\begin{equation}
F-Measure=2 * \frac{\text{precision}*\text{recall}}{\text{precision}+\text{recall}}
\end{equation}

During the recommender development, Mahout provides a variety of similarity and neighborhood functions to choose from. Depending on the applied functions, the recommender computes the similarity between two items differently and considers different items to be suitable for a similarity measure. Using the evaluation results, the combinations of the following similarity and neighborhood functions can be tested to determine which one of them performs best on the given data.  The following table shows the f-measures of the 16 investigated combinations:

\tablaSmall{F-Measures of the recommenders using different similarity and neighborhood functions}{p{3cm} p{1.5cm} p{1.5cm} p{1.5cm} p{1.5cm}}{f-measurerecommendation}
{ \multicolumn{1}{l}{ } & Threshold User & Nearest 2 User & Nearest 5 User & Nearest 10 User \\}{ 
Euclidean Distance & 0.5 & 0.4 & 0.5 & 0.5 \\
Pearson Correlation & NaN & NaN & NaN & NaN \\
Loglikelihood & 0.55 & 0.29 & 0.73 & 0.5 \\
Spearman Correlation & NaN & NaN & NaN & NaN \\
} 

It quickly becomes clear that the Pearson Correlation Similarity and Spearman Correlation Similarity are not suitable for this recommender as they do not output valid performance results at all. A reason for this is that the applied user data is too sparse to achieve significant results using these similarity functions. The best f-measure is achieved by a recommender using Loglikelihood Similarity and Nearest-5-User as neighborhood function. Examining this recommender’s performance in detail, we see that it achieves a precision value of 0.8 and a recall of 0.67.
As we can see, the precision value is higher than the recall value. The precision tells us the fraction of retrieved items that are relevant whereas the recall tells us the fraction of relevant items that are retrieved. In this project, the precision value is considered as more important than the recall value as the user has to be provided with recommendations that fit his interests. Yet, the fact that the user-based recommender may not find all possible recommendations for the user, is rather negligible as the content-based mechanism is used as a fallback in this case. Also it is assumed that the overall performance of the recommender will rise with increasing data as more users use the chatbot (cold start problem of a recommender).
